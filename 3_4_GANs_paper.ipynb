{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee8a391",
   "metadata": {},
   "source": [
    "### GANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d7dd6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "147ddf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f6f619d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/processed_files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed_files'"
     ]
    }
   ],
   "source": [
    "os.chdir(path = \"data/processed_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "62f702ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ['/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/BOVESPA_train.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/FTSE_train.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/MSCI_train.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/NIFTY50_train.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/SHANGHAI_train.parquet']\n",
      "Val:   ['/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/BOVESPA_valid.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/FTSE_valid.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/MSCI_valid.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/NIFTY50_valid.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/SHANGHAI_valid.parquet']\n",
      "Test:  ['/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/BOVESPA_test.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/FTSE_test.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/MSCI_test.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/NIFTY50_test.parquet', '/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/SHANGHAI_test.parquet']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.getcwd()\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_DIR, \"valid\")\n",
    "TEST_DIR  = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "def list_datasets(path):\n",
    "    return sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".parquet\")])\n",
    "\n",
    "train_datasets = list_datasets(TRAIN_DIR)\n",
    "val_datasets   = list_datasets(VAL_DIR)\n",
    "test_datasets  = list_datasets(TEST_DIR)\n",
    "\n",
    "print(\"Train:\", train_datasets)\n",
    "print(\"Val:  \", val_datasets)\n",
    "print(\"Test: \", test_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b9e20",
   "metadata": {},
   "source": [
    "Base Model Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f6509101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGAN:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        \"\"\"Train model and update weights.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def validate(self, val_data):\n",
    "        \"\"\"Return validation metric used to tune hyperparameters.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate synthetic data.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6bb3b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, seq_len):\n",
    "    X = []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X.append(data[i:i+seq_len])\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99104ea4",
   "metadata": {},
   "source": [
    "Metric Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "48026519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def compute_metrics(real, synthetic):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(real.flatten(), synthetic.flatten()),\n",
    "        \"KS\": np.mean([\n",
    "            ks_2samp(real[:, i], synthetic[:, i]).pvalue\n",
    "            for i in range(real.shape[1])\n",
    "        ])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a1d39",
   "metadata": {},
   "source": [
    "GAN Placeholders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e05d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeGAN(BaseGAN):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.trained_data = None\n",
    "    \n",
    "#     def train(self, train_data):\n",
    "#         print(\"Training TimeGAN...\")\n",
    "#         self.trained_data = np.asarray(train_data, dtype=np.float32)\n",
    "    \n",
    "#     def validate(self, val_data):\n",
    "#         return np.random.random()  # placeholder\n",
    "    \n",
    "#     def generate(self, n_samples):\n",
    "#         if self.trained_data is None:\n",
    "#             raise RuntimeError(\"Model not trained yet\")\n",
    "#         # Return random samples from training data as placeholder\n",
    "#         if len(self.trained_data) >= n_samples:\n",
    "#             return self.trained_data[:n_samples]\n",
    "#         else:\n",
    "#             # Repeat if not enough samples\n",
    "#             return np.tile(self.trained_data, (n_samples // len(self.trained_data) + 1, 1))[:n_samples]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040499b",
   "metadata": {},
   "source": [
    "### TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "393fbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------------\n",
    "# Embedder Network (RNN-based)\n",
    "# -------------------------------\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        h, _ = self.rnn(x)\n",
    "        h = self.sigmoid(self.fc(h))\n",
    "        return h\n",
    "\n",
    "# -------------------------------\n",
    "# Recovery Network (RNN-based)\n",
    "# -------------------------------\n",
    "class Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            hidden_dim, hidden_dim, num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (batch, seq_len, hidden_dim)\n",
    "        x, _ = self.rnn(h)\n",
    "        x = self.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Generator Network (RNN-based)\n",
    "# -------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, hidden_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            noise_dim, hidden_dim, num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: (batch, seq_len, noise_dim)\n",
    "        h, _ = self.rnn(z)\n",
    "        h = self.sigmoid(self.fc(h))\n",
    "        return h\n",
    "\n",
    "# -------------------------------\n",
    "# Supervisor Network (for step-ahead prediction)\n",
    "# -------------------------------\n",
    "class Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            hidden_dim, hidden_dim, num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (batch, seq_len, hidden_dim)\n",
    "        h_out, _ = self.rnn(h)\n",
    "        h_out = self.sigmoid(self.fc(h_out))\n",
    "        return h_out\n",
    "\n",
    "# -------------------------------\n",
    "# Discriminator Network (RNN-based)\n",
    "# -------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            hidden_dim, hidden_dim, num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        # h: (batch, seq_len, hidden_dim)\n",
    "        h_out, _ = self.rnn(h)\n",
    "        y = self.fc(h_out)  # (batch, seq_len, 1)\n",
    "        return y\n",
    "\n",
    "# -------------------------------\n",
    "# TimeGAN Implementation\n",
    "# -------------------------------\n",
    "class TimeGAN(BaseGAN):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config or {})\n",
    "        cfg = self.config\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.hidden_dim = cfg.get(\"hidden_dim\", 24)\n",
    "        self.noise_dim = cfg.get(\"noise_dim\", 24)\n",
    "        self.num_layers = cfg.get(\"num_layers\", 3)\n",
    "        self.lr = cfg.get(\"lr\", 1e-3)\n",
    "        self.batch_size = cfg.get(\"batch_size\", 128)\n",
    "        self.epochs = cfg.get(\"epochs\", 50)\n",
    "        \n",
    "        # Training iterations for each phase\n",
    "        self.iterations = cfg.get(\"iterations\", 10000)\n",
    "\n",
    "        # Data-dependent\n",
    "        self.seq_len = None\n",
    "        self.n_features = None\n",
    "        self.data_min = None\n",
    "        self.data_max = None\n",
    "        self.data_mean = None\n",
    "        self.data_std = None\n",
    "\n",
    "        # Networks\n",
    "        self.embedder = None\n",
    "        self.recovery = None\n",
    "        self.generator = None\n",
    "        self.supervisor = None\n",
    "        self.discriminator = None\n",
    "        \n",
    "        # Optimizers\n",
    "        self.opt_autoencoder = None\n",
    "        self.opt_supervisor = None\n",
    "        self.opt_generator = None\n",
    "        self.opt_discriminator = None\n",
    "\n",
    "    def _normalize_data(self, data, fit=True):\n",
    "        if fit:\n",
    "            self.data_min = np.min(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_max = np.max(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_mean = np.mean(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_std = np.std(data, axis=(0, 1), keepdims=True) + 1e-8\n",
    "            \n",
    "            print(f\"[TimeGAN] Data range: [{data.min():.4f}, {data.max():.4f}]\")\n",
    "            print(f\"[TimeGAN] Mean: {self.data_mean.squeeze():.6f}, Std: {self.data_std.squeeze():.6f}\")\n",
    "        \n",
    "        # Min-max normalization to [0, 1] (TimeGAN uses sigmoid activations)\n",
    "        data_range = self.data_max - self.data_min + 1e-8\n",
    "        normalized = (data - self.data_min) / data_range\n",
    "        normalized = np.clip(normalized, 0.0, 1.0)\n",
    "        \n",
    "        if fit:\n",
    "            print(f\"[TimeGAN] Normalized range: [{normalized.min():.4f}, {normalized.max():.4f}]\")\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "    def _denormalize_data(self, data):\n",
    "        data_range = self.data_max - self.data_min\n",
    "        return data * data_range + self.data_min\n",
    "\n",
    "    def _make_windows(self, series, seq_len):\n",
    "        T = series.shape[0]\n",
    "        if T < seq_len:\n",
    "            raise ValueError(f\"Time series too short ({T}) for seq_len={seq_len}\")\n",
    "        \n",
    "        windows = []\n",
    "        for i in range(T - seq_len + 1):\n",
    "            windows.append(series[i:i+seq_len])\n",
    "        \n",
    "        return np.array(windows)\n",
    "\n",
    "    def _build_models(self, seq_len, n_features):\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # Build networks\n",
    "        self.embedder = Embedder(n_features, self.hidden_dim, self.num_layers).to(self.device)\n",
    "        self.recovery = Recovery(self.hidden_dim, n_features, self.num_layers).to(self.device)\n",
    "        self.generator = Generator(self.noise_dim, self.hidden_dim, self.num_layers).to(self.device)\n",
    "        self.supervisor = Supervisor(self.hidden_dim, 2).to(self.device)\n",
    "        self.discriminator = Discriminator(self.hidden_dim, self.num_layers).to(self.device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.opt_autoencoder = optim.Adam(\n",
    "            list(self.embedder.parameters()) + list(self.recovery.parameters()),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        self.opt_supervisor = optim.Adam(\n",
    "            list(self.supervisor.parameters()) + list(self.generator.parameters()),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        self.opt_generator = optim.Adam(\n",
    "            list(self.generator.parameters()) + list(self.supervisor.parameters()),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        self.opt_discriminator = optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        \n",
    "        print(f\"[TimeGAN] Models built:\")\n",
    "        print(f\"  Embedder: {sum(p.numel() for p in self.embedder.parameters()):,}\")\n",
    "        print(f\"  Recovery: {sum(p.numel() for p in self.recovery.parameters()):,}\")\n",
    "        print(f\"  Generator: {sum(p.numel() for p in self.generator.parameters()):,}\")\n",
    "        print(f\"  Supervisor: {sum(p.numel() for p in self.supervisor.parameters()):,}\")\n",
    "        print(f\"  Discriminator: {sum(p.numel() for p in self.discriminator.parameters()):,}\")\n",
    "\n",
    "    def train(self, train_data):\n",
    "        data = np.asarray(train_data, dtype=np.float32)\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "        \n",
    "        # Handle NaN\n",
    "        if np.isnan(data).any():\n",
    "            nan_count = np.isnan(data).sum()\n",
    "            print(f\"[TimeGAN] WARNING: Found {nan_count} NaN values, handling...\")\n",
    "            data_df = pd.DataFrame(data)\n",
    "            data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "            data = data_df.values\n",
    "            print(f\"[TimeGAN] NaN handling complete\")\n",
    "        \n",
    "        T, n_features = data.shape\n",
    "        \n",
    "        # Choose seq_len\n",
    "        max_seq_len = T\n",
    "        if max_seq_len >= 256:\n",
    "            seq_len = 256\n",
    "        elif max_seq_len >= 128:\n",
    "            seq_len = 128\n",
    "        elif max_seq_len >= 64:\n",
    "            seq_len = 64\n",
    "        elif max_seq_len >= 32:\n",
    "            seq_len = 32\n",
    "        elif max_seq_len >= 24:\n",
    "            seq_len = 24\n",
    "        else:\n",
    "            seq_len = max(16, max_seq_len)\n",
    "        \n",
    "        print(f\"[TimeGAN] Creating windows: T={T}, seq_len={seq_len}, n_features={n_features}\")\n",
    "        \n",
    "        data = self._make_windows(data, seq_len)\n",
    "        data = self._normalize_data(data, fit=True)\n",
    "        \n",
    "        n_windows = data.shape[0]\n",
    "        print(f\"[TimeGAN] Created {n_windows} windows\")\n",
    "\n",
    "        if self.embedder is None:\n",
    "            self._build_models(seq_len, n_features)\n",
    "\n",
    "        # Convert to tensors\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # ===================================\n",
    "        # Phase 1: Train Embedder & Recovery\n",
    "        # ===================================\n",
    "        print(\"[TimeGAN] Phase 1: Training Autoencoder...\")\n",
    "        for epoch in range(self.epochs // 2):\n",
    "            self.embedder.train()\n",
    "            self.recovery.train()\n",
    "            \n",
    "            # Sample random batch\n",
    "            idx = np.random.permutation(n_windows)[:self.batch_size]\n",
    "            X = data_tensor[idx]\n",
    "            \n",
    "            # Forward\n",
    "            H = self.embedder(X)\n",
    "            X_tilde = self.recovery(H)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            loss_reconstruction = nn.MSELoss()(X, X_tilde)\n",
    "            \n",
    "            # Backward\n",
    "            self.opt_autoencoder.zero_grad()\n",
    "            loss_reconstruction.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.embedder.parameters()) + list(self.recovery.parameters()),\n",
    "                max_norm=1.0\n",
    "            )\n",
    "            self.opt_autoencoder.step()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{self.epochs//2} | Reconstruction Loss: {loss_reconstruction.item():.4f}\")\n",
    "        \n",
    "        # ===================================\n",
    "        # Phase 2: Train Supervisor\n",
    "        # ===================================\n",
    "        print(\"[TimeGAN] Phase 2: Training Supervisor...\")\n",
    "        for epoch in range(self.epochs // 2):\n",
    "            self.supervisor.train()\n",
    "            self.generator.train()\n",
    "            \n",
    "            idx = np.random.permutation(n_windows)[:self.batch_size]\n",
    "            X = data_tensor[idx]\n",
    "            \n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                H = self.embedder(X)\n",
    "            \n",
    "            # Supervised loss: predict next step\n",
    "            H_supervise = self.supervisor(H)\n",
    "            loss_supervisor = nn.MSELoss()(H[:, 1:, :], H_supervise[:, :-1, :])\n",
    "            \n",
    "            # Backward\n",
    "            self.opt_supervisor.zero_grad()\n",
    "            loss_supervisor.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.supervisor.parameters()) + list(self.generator.parameters()),\n",
    "                max_norm=1.0\n",
    "            )\n",
    "            self.opt_supervisor.step()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{self.epochs//2} | Supervisor Loss: {loss_supervisor.item():.4f}\")\n",
    "        \n",
    "        # ===================================\n",
    "        # Phase 3: Joint Training (GAN)\n",
    "        # ===================================\n",
    "        print(\"[TimeGAN] Phase 3: Joint Adversarial Training...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            g_losses = []\n",
    "            d_losses = []\n",
    "            \n",
    "            for _ in range(min(6, n_windows // self.batch_size)):\n",
    "                idx = np.random.permutation(n_windows)[:self.batch_size]\n",
    "                X = data_tensor[idx]\n",
    "                \n",
    "                # ===== Train Discriminator =====\n",
    "                self.discriminator.train()\n",
    "                \n",
    "                # Real data\n",
    "                with torch.no_grad():\n",
    "                    H_real = self.embedder(X)\n",
    "                \n",
    "                # Fake data\n",
    "                Z = torch.randn(self.batch_size, self.seq_len, self.noise_dim).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    E_hat = self.generator(Z)\n",
    "                    H_hat = self.supervisor(E_hat)\n",
    "                \n",
    "                # Discriminator loss\n",
    "                y_real = self.discriminator(H_real)\n",
    "                y_fake = self.discriminator(H_hat)\n",
    "                \n",
    "                d_loss_real = nn.BCEWithLogitsLoss()(y_real, torch.ones_like(y_real))\n",
    "                d_loss_fake = nn.BCEWithLogitsLoss()(y_fake, torch.zeros_like(y_fake))\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "                \n",
    "                self.opt_discriminator.zero_grad()\n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
    "                self.opt_discriminator.step()\n",
    "                \n",
    "                d_losses.append(d_loss.item())\n",
    "                \n",
    "                # ===== Train Generator =====\n",
    "                self.generator.train()\n",
    "                self.supervisor.train()\n",
    "                \n",
    "                # Generate fake data\n",
    "                Z = torch.randn(self.batch_size, self.seq_len, self.noise_dim).to(self.device)\n",
    "                E_hat = self.generator(Z)\n",
    "                H_hat = self.supervisor(E_hat)\n",
    "                \n",
    "                # Reconstruct\n",
    "                with torch.no_grad():\n",
    "                    H_real = self.embedder(X)\n",
    "                H_supervise = self.supervisor(H_real)\n",
    "                \n",
    "                # Generator losses\n",
    "                y_fake_g = self.discriminator(H_hat)\n",
    "                g_loss_u = nn.BCEWithLogitsLoss()(y_fake_g, torch.ones_like(y_fake_g))\n",
    "                g_loss_s = nn.MSELoss()(H_real[:, 1:, :], H_supervise[:, :-1, :])\n",
    "                \n",
    "                # Moment matching loss\n",
    "                g_loss_v1 = torch.mean(torch.abs(torch.mean(H_real, dim=0) - torch.mean(H_hat, dim=0)))\n",
    "                g_loss_v2 = torch.mean(torch.abs(torch.std(H_real, dim=0) - torch.std(H_hat, dim=0)))\n",
    "                g_loss_v = g_loss_v1 + g_loss_v2\n",
    "                \n",
    "                # Total generator loss\n",
    "                g_loss = g_loss_u + 100 * torch.sqrt(g_loss_s) + 100 * g_loss_v\n",
    "                \n",
    "                self.opt_generator.zero_grad()\n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.generator.parameters()) + list(self.supervisor.parameters()),\n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                self.opt_generator.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "            \n",
    "            if len(d_losses) > 0 and len(g_losses) > 0:\n",
    "                avg_d = np.mean(d_losses)\n",
    "                avg_g = np.mean(g_losses)\n",
    "                \n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                    print(f\"[TimeGAN] Epoch {epoch+1}/{self.epochs} | D={avg_d:.4f} | G={avg_g:.4f}\")\n",
    "\n",
    "    def validate(self, val_data):\n",
    "        data = np.asarray(val_data, dtype=np.float32)\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "        \n",
    "        if np.isnan(data).any():\n",
    "            data_df = pd.DataFrame(data)\n",
    "            data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "            data = data_df.values\n",
    "        \n",
    "        if self.embedder is None:\n",
    "            raise RuntimeError(\"Model not trained yet\")\n",
    "        \n",
    "        T = data.shape[0]\n",
    "        \n",
    "        if T < self.seq_len:\n",
    "            temp_seq_len = max(16, T)\n",
    "            if temp_seq_len < 16:\n",
    "                return 0.0\n",
    "            \n",
    "            data = self._make_windows(data, temp_seq_len)\n",
    "            data = self._normalize_data(data, fit=False)\n",
    "            \n",
    "            n_windows, _, n_features = data.shape\n",
    "            padded_data = np.zeros((n_windows, self.seq_len, n_features), dtype=np.float32)\n",
    "            padded_data[:, :temp_seq_len, :] = data\n",
    "            data = padded_data\n",
    "        else:\n",
    "            data = self._make_windows(data, self.seq_len)\n",
    "            data = self._normalize_data(data, fit=False)\n",
    "        \n",
    "        real = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.embedder.eval()\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = min(self.batch_size, real.size(0))\n",
    "            H_real = self.embedder(real[:batch_size])\n",
    "            y_real = self.discriminator(H_real)\n",
    "            score = torch.sigmoid(y_real).mean().item()\n",
    "        \n",
    "        self.embedder.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def generate(self, n_samples):\n",
    "        if self.generator is None:\n",
    "            raise RuntimeError(\"Model not trained yet\")\n",
    "\n",
    "        self.generator.eval()\n",
    "        self.supervisor.eval()\n",
    "        self.recovery.eval()\n",
    "        \n",
    "        n_windows = max(1, (n_samples + self.seq_len - 1) // self.seq_len)\n",
    "        \n",
    "        out = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_windows, self.batch_size):\n",
    "                b = min(self.batch_size, n_windows - i)\n",
    "                Z = torch.randn(b, self.seq_len, self.noise_dim).to(self.device)\n",
    "                E_hat = self.generator(Z)\n",
    "                H_hat = self.supervisor(E_hat)\n",
    "                X_hat = self.recovery(H_hat)\n",
    "                out.append(X_hat.cpu().numpy())\n",
    "\n",
    "        self.generator.train()\n",
    "        self.supervisor.train()\n",
    "        self.recovery.train()\n",
    "        \n",
    "        windows = np.concatenate(out, axis=0)\n",
    "        windows = self._denormalize_data(windows)\n",
    "        \n",
    "        reconstructed = windows.reshape(-1, self.n_features)\n",
    "        reconstructed = reconstructed[:n_samples]\n",
    "        \n",
    "        if reconstructed.shape[0] < n_samples:\n",
    "            padding = np.repeat(reconstructed[-1:], n_samples - reconstructed.shape[0], axis=0)\n",
    "            reconstructed = np.vstack([reconstructed, padding])\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd491d",
   "metadata": {},
   "source": [
    "### QuantGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a7a295a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------------\n",
    "# Causal Convolution with proper length preservation\n",
    "# -------------------------------\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                             padding=self.padding, dilation=dilation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply conv with left padding\n",
    "        out = self.conv(x)\n",
    "        # Remove right padding to maintain causality and original length\n",
    "        if self.padding > 0:\n",
    "            out = out[:, :, :-self.padding]\n",
    "        return out\n",
    "\n",
    "# -------------------------------\n",
    "# Temporal Block with Causal Convolutions\n",
    "# -------------------------------\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = CausalConv1d(in_channels, out_channels, kernel_size, dilation)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = CausalConv1d(out_channels, out_channels, kernel_size, dilation)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            self.conv1, self.bn1, self.relu1, self.dropout1,\n",
    "            self.conv2, self.bn2, self.relu2, self.dropout2\n",
    "        )\n",
    "        \n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "# -------------------------------\n",
    "# Temporal Convolutional Network\n",
    "# -------------------------------\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers.append(TemporalBlock(\n",
    "                in_channels, out_channels, kernel_size, \n",
    "                dilation=dilation_size, dropout=dropout\n",
    "            ))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# -------------------------------\n",
    "# QuantGAN Generator (TCN-based)\n",
    "# -------------------------------\n",
    "class QuantGAN_Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, seq_len, n_features, hidden_channels=[64, 64, 32]):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Project noise to initial sequence\n",
    "        self.fc = nn.Linear(noise_dim, hidden_channels[0] * (seq_len // 4))\n",
    "        \n",
    "        # Upsample to target length\n",
    "        self.upsample1 = nn.ConvTranspose1d(hidden_channels[0], hidden_channels[0], 4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.upsample2 = nn.ConvTranspose1d(hidden_channels[0], hidden_channels[1], 4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # TCN for temporal dependencies\n",
    "        self.tcn = TemporalConvNet(\n",
    "            num_inputs=hidden_channels[1],\n",
    "            num_channels=hidden_channels[1:],\n",
    "            kernel_size=3,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Final projection\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv1d(hidden_channels[-1], n_features, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        # Project and reshape\n",
    "        x = self.fc(z)\n",
    "        x = x.view(batch_size, -1, self.seq_len // 4)\n",
    "        \n",
    "        # Upsample\n",
    "        x = self.relu1(self.bn1(self.upsample1(x)))\n",
    "        x = self.relu2(self.bn2(self.upsample2(x)))\n",
    "        \n",
    "        # Ensure correct length\n",
    "        if x.size(2) != self.seq_len:\n",
    "            x = x[:, :, :self.seq_len]\n",
    "        \n",
    "        # Apply TCN (maintains length due to causal padding)\n",
    "        x = self.tcn(x)\n",
    "        \n",
    "        # Final adjustment to ensure exact length\n",
    "        if x.size(2) != self.seq_len:\n",
    "            x = x[:, :, :self.seq_len]\n",
    "        \n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "# -------------------------------\n",
    "# QuantGAN Discriminator (TCN-based)\n",
    "# -------------------------------\n",
    "class QuantGAN_Discriminator(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, hidden_channels=[32, 64, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tcn = TemporalConvNet(\n",
    "            num_inputs=n_features,\n",
    "            num_channels=hidden_channels,\n",
    "            kernel_size=3,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels[-1], 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.tcn(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# QuantGAN (WGAN-GP with TCN)\n",
    "# -------------------------------\n",
    "class QuantGAN(BaseGAN):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config or {})\n",
    "        cfg = self.config\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.noise_dim = cfg.get(\"noise_dim\", 100)\n",
    "        self.lr_g = cfg.get(\"lr_g\", 1e-4)\n",
    "        self.lr_d = cfg.get(\"lr_d\", 1e-4)\n",
    "        self.batch_size = cfg.get(\"batch_size\", 64)\n",
    "        self.epochs = cfg.get(\"epochs\", 50)\n",
    "        self.lambda_gp = cfg.get(\"lambda_gp\", 10.0)\n",
    "        self.n_critic = cfg.get(\"n_critic\", 3)\n",
    "        self.hidden_channels_g = cfg.get(\"hidden_channels_g\", [64, 64, 32])\n",
    "        self.hidden_channels_d = cfg.get(\"hidden_channels_d\", [32, 64, 128])\n",
    "\n",
    "        self.seq_len = None\n",
    "        self.n_features = None\n",
    "        self.data_min = None\n",
    "        self.data_max = None\n",
    "        self.data_mean = None\n",
    "        self.data_std = None\n",
    "\n",
    "        self.G = None\n",
    "        self.D = None\n",
    "        self.opt_G = None\n",
    "        self.opt_D = None\n",
    "\n",
    "    def _normalize_data(self, data, fit=True):\n",
    "        if fit:\n",
    "            self.data_min = np.min(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_max = np.max(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_mean = np.mean(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_std = np.std(data, axis=(0, 1), keepdims=True) + 1e-8\n",
    "            \n",
    "            print(f\"[QuantGAN] Data range: [{data.min():.4f}, {data.max():.4f}]\")\n",
    "            print(f\"[QuantGAN] Mean: {self.data_mean.squeeze():.6f}, Std: {self.data_std.squeeze():.6f}\")\n",
    "        \n",
    "        data_range = self.data_max - self.data_min + 1e-8\n",
    "        normalized = 2 * (data - self.data_min) / data_range - 1\n",
    "        normalized = np.clip(normalized, -0.99, 0.99)\n",
    "        \n",
    "        if fit:\n",
    "            print(f\"[QuantGAN] Normalized range: [{normalized.min():.4f}, {normalized.max():.4f}]\")\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "    def _denormalize_data(self, data):\n",
    "        data_range = self.data_max - self.data_min\n",
    "        return (data + 1) / 2 * data_range + self.data_min\n",
    "\n",
    "    def _make_windows(self, series, seq_len):\n",
    "        T = series.shape[0]\n",
    "        if T < seq_len:\n",
    "            raise ValueError(f\"Time series too short ({T}) for seq_len={seq_len}\")\n",
    "        \n",
    "        windows = []\n",
    "        for i in range(T - seq_len + 1):\n",
    "            windows.append(series[i:i+seq_len])\n",
    "        \n",
    "        return np.array(windows)\n",
    "\n",
    "    def _build_models(self, seq_len, n_features):\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.G = QuantGAN_Generator(\n",
    "            self.noise_dim, seq_len, n_features, self.hidden_channels_g\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.D = QuantGAN_Discriminator(\n",
    "            seq_len, n_features, self.hidden_channels_d\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.opt_G = optim.Adam(self.G.parameters(), lr=self.lr_g, betas=(0.0, 0.9))\n",
    "        self.opt_D = optim.Adam(self.D.parameters(), lr=self.lr_d, betas=(0.0, 0.9))\n",
    "        \n",
    "        self._init_weights(self.G)\n",
    "        self._init_weights(self.D)\n",
    "        \n",
    "        print(f\"[QuantGAN] Models built - G params: {sum(p.numel() for p in self.G.parameters()):,}\")\n",
    "        print(f\"[QuantGAN] Models built - D params: {sum(p.numel() for p in self.D.parameters()):,}\")\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):\n",
    "            if hasattr(m, 'weight') and m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _gradient_penalty(self, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1, device=self.device)\n",
    "        \n",
    "        interp = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "        d_interp = self.D(interp)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interp,\n",
    "            inputs=interp,\n",
    "            grad_outputs=torch.ones_like(d_interp),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.reshape(batch_size, -1)\n",
    "        grad_norm = gradients.norm(2, dim=1)\n",
    "        penalty = torch.mean((grad_norm - 1) ** 2)\n",
    "        \n",
    "        return penalty\n",
    "\n",
    "    def train(self, train_data):\n",
    "        data = np.asarray(train_data, dtype=np.float32)\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "        \n",
    "        if np.isnan(data).any():\n",
    "            nan_count = np.isnan(data).sum()\n",
    "            print(f\"[QuantGAN] WARNING: Found {nan_count} NaN values, handling...\")\n",
    "            data_df = pd.DataFrame(data)\n",
    "            data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "            data = data_df.values\n",
    "            print(f\"[QuantGAN] NaN handling complete\")\n",
    "        \n",
    "        T, n_features = data.shape\n",
    "        \n",
    "        max_seq_len = (T // 4) * 4\n",
    "        if max_seq_len < 16:\n",
    "            raise ValueError(f\"Time series too short ({T}) for QuantGAN\")\n",
    "        \n",
    "        if max_seq_len >= 256:\n",
    "            seq_len = 256\n",
    "        elif max_seq_len >= 128:\n",
    "            seq_len = 128\n",
    "        elif max_seq_len >= 64:\n",
    "            seq_len = 64\n",
    "        elif max_seq_len >= 32:\n",
    "            seq_len = 32\n",
    "        else:\n",
    "            seq_len = max_seq_len\n",
    "        \n",
    "        print(f\"[QuantGAN] Creating windows: T={T}, seq_len={seq_len}, n_features={n_features}\")\n",
    "        \n",
    "        data = self._make_windows(data, seq_len)\n",
    "        data = self._normalize_data(data, fit=True)\n",
    "        \n",
    "        n_windows = data.shape[0]\n",
    "        print(f\"[QuantGAN] Created {n_windows} windows\")\n",
    "\n",
    "        if self.G is None:\n",
    "            self._build_models(seq_len, n_features)\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(data, dtype=torch.float32))\n",
    "        loader = DataLoader(dataset, self.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        print(f\"[QuantGAN] Starting training with {len(loader)} batches per epoch\")\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            d_losses = []\n",
    "            g_losses = []\n",
    "            gp_losses = []\n",
    "            \n",
    "            for batch_idx, (real,) in enumerate(loader):\n",
    "                real = real.to(self.device)\n",
    "                batch_size = real.size(0)\n",
    "\n",
    "                for _ in range(self.n_critic):\n",
    "                    self.opt_D.zero_grad()\n",
    "                    \n",
    "                    z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        fake = self.G(z)\n",
    "                    \n",
    "                    d_real = self.D(real)\n",
    "                    d_fake = self.D(fake)\n",
    "                    \n",
    "                    d_loss = d_fake.mean() - d_real.mean()\n",
    "                    gp = self._gradient_penalty(real, fake)\n",
    "                    d_total = d_loss + self.lambda_gp * gp\n",
    "                    \n",
    "                    if not (torch.isnan(d_total) or torch.isinf(d_total) or abs(d_total.item()) > 1e6):\n",
    "                        d_total.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(self.D.parameters(), max_norm=1.0)\n",
    "                        self.opt_D.step()\n",
    "                \n",
    "                d_losses.append(d_total.item())\n",
    "                gp_losses.append(gp.item())\n",
    "\n",
    "                self.opt_G.zero_grad()\n",
    "                z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "                fake = self.G(z)\n",
    "                g_loss = -self.D(fake).mean()\n",
    "                \n",
    "                if not (torch.isnan(g_loss) or torch.isinf(g_loss) or abs(g_loss.item()) > 1e6):\n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.G.parameters(), max_norm=1.0)\n",
    "                    self.opt_G.step()\n",
    "                    g_losses.append(g_loss.item())\n",
    "\n",
    "            if len(d_losses) > 0 and len(g_losses) > 0:\n",
    "                avg_d = np.mean(d_losses)\n",
    "                avg_g = np.mean(g_losses)\n",
    "                avg_gp = np.mean(gp_losses)\n",
    "                \n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                    print(f\"[QuantGAN] Epoch {epoch+1}/{self.epochs} | D={avg_d:.4f} | G={avg_g:.4f} | GP={avg_gp:.4f}\")\n",
    "\n",
    "    def validate(self, val_data):\n",
    "        data = np.asarray(val_data, dtype=np.float32)\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "        \n",
    "        if np.isnan(data).any():\n",
    "            data_df = pd.DataFrame(data)\n",
    "            data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "            data = data_df.values\n",
    "        \n",
    "        if self.seq_len is None:\n",
    "            raise RuntimeError(\"Model not trained yet\")\n",
    "        \n",
    "        T = data.shape[0]\n",
    "        \n",
    "        if T < self.seq_len:\n",
    "            temp_seq_len = (T // 4) * 4\n",
    "            if temp_seq_len < 16:\n",
    "                return 0.0\n",
    "            \n",
    "            data = self._make_windows(data, temp_seq_len)\n",
    "            data = self._normalize_data(data, fit=False)\n",
    "            \n",
    "            n_windows, _, n_features = data.shape\n",
    "            padded_data = np.zeros((n_windows, self.seq_len, n_features), dtype=np.float32)\n",
    "            padded_data[:, :temp_seq_len, :] = data\n",
    "            data = padded_data\n",
    "        else:\n",
    "            data = self._make_windows(data, self.seq_len)\n",
    "            data = self._normalize_data(data, fit=False)\n",
    "        \n",
    "        real = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.G.eval()\n",
    "        self.D.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = min(self.batch_size, real.size(0))\n",
    "            z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "            fake = self.G(z)\n",
    "            score = (self.D(real[:batch_size]).mean() - self.D(fake).mean()).item()\n",
    "        \n",
    "        self.G.train()\n",
    "        self.D.train()\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def generate(self, n_samples):\n",
    "        if self.G is None:\n",
    "            raise RuntimeError(\"Model not trained yet\")\n",
    "\n",
    "        self.G.eval()\n",
    "        \n",
    "        n_windows = max(1, (n_samples + self.seq_len - 1) // self.seq_len)\n",
    "        \n",
    "        out = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_windows, self.batch_size):\n",
    "                b = min(self.batch_size, n_windows - i)\n",
    "                z = torch.randn(b, self.noise_dim, device=self.device)\n",
    "                fake_windows = self.G(z).cpu().numpy()\n",
    "                out.append(fake_windows)\n",
    "\n",
    "        self.G.train()\n",
    "        windows = np.concatenate(out, axis=0)\n",
    "        \n",
    "        windows = self._denormalize_data(windows)\n",
    "        reconstructed = windows.reshape(-1, self.n_features)\n",
    "        reconstructed = reconstructed[:n_samples]\n",
    "        \n",
    "        if reconstructed.shape[0] < n_samples:\n",
    "            padding = np.repeat(reconstructed[-1:], n_samples - reconstructed.shape[0], axis=0)\n",
    "            reconstructed = np.vstack([reconstructed, padding])\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3cb7d",
   "metadata": {},
   "source": [
    "### FINGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b6bec387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------------\n",
    "# Residual Block\n",
    "# -------------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm1d(channels)\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(x + self.block(x))\n",
    "\n",
    "# -------------------------------\n",
    "# Generator (Deconv CNN)\n",
    "# -------------------------------\n",
    "class FinGAN_Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, seq_len, n_features, base_channels=64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 8 == 0, \"seq_len must be divisible by 8\"\n",
    "\n",
    "        self.start_len = seq_len // 8\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, base_channels * self.start_len),\n",
    "            nn.BatchNorm1d(base_channels * self.start_len),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base_channels, base_channels // 2, 4, 2, 1),\n",
    "            nn.BatchNorm1d(base_channels // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(base_channels // 2, base_channels // 4, 4, 2, 1),\n",
    "            nn.BatchNorm1d(base_channels // 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose1d(base_channels // 4, n_features, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), -1, self.start_len)\n",
    "        x = self.net(x)\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "# -------------------------------\n",
    "# Critic (CNN)\n",
    "# -------------------------------\n",
    "class FinGAN_Critic(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, base_channels=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(n_features, base_channels, 5, padding=2),\n",
    "            nn.LayerNorm([base_channels, seq_len]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(base_channels, base_channels * 2, 4, stride=2, padding=1),\n",
    "            nn.LayerNorm([base_channels * 2, seq_len // 2]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(base_channels * 2, base_channels * 4, 4, stride=2, padding=1),\n",
    "            nn.LayerNorm([base_channels * 4, seq_len // 4]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((seq_len // 4) * base_channels * 4, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.permute(0, 2, 1))\n",
    "\n",
    "# -------------------------------\n",
    "# FIN-GAN (WGAN-GP) - inherits from BaseGAN\n",
    "# -------------------------------\n",
    "class FinGAN(BaseGAN):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config or {})\n",
    "        cfg = self.config\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.noise_dim = cfg.get(\"noise_dim\", 100)\n",
    "        self.lr_g = cfg.get(\"lr_g\", 1e-4)\n",
    "        self.lr_d = cfg.get(\"lr_d\", 1e-4)\n",
    "        self.batch_size = cfg.get(\"batch_size\", 64)\n",
    "        self.epochs = cfg.get(\"epochs\", 50)\n",
    "        self.lambda_gp = cfg.get(\"lambda_gp\", 10.0)\n",
    "        self.n_critic = cfg.get(\"n_critic\", 3)\n",
    "        self.base_channels = cfg.get(\"base_channels\", 32)\n",
    "\n",
    "        self.seq_len = None\n",
    "        self.n_features = None\n",
    "        self.data_min = None\n",
    "        self.data_max = None\n",
    "        self.data_mean = None\n",
    "        self.data_std = None\n",
    "\n",
    "        self.G = None\n",
    "        self.D = None\n",
    "        self.opt_G = None\n",
    "        self.opt_D = None\n",
    "\n",
    "    def _normalize_data(self, data, fit=True):\n",
    "        if fit:\n",
    "            self.data_min = np.min(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_max = np.max(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_mean = np.mean(data, axis=(0, 1), keepdims=True)\n",
    "            self.data_std = np.std(data, axis=(0, 1), keepdims=True) + 1e-8\n",
    "            \n",
    "            print(f\"[FinGAN] Data range: [{data.min():.4f}, {data.max():.4f}]\")\n",
    "            print(f\"[FinGAN] Mean: {self.data_mean.squeeze():.6f}, Std: {self.data_std.squeeze():.6f}\")\n",
    "        \n",
    "        data_range = self.data_max - self.data_min + 1e-8\n",
    "        normalized = 2 * (data - self.data_min) / data_range - 1\n",
    "        normalized = np.clip(normalized, -0.99, 0.99)\n",
    "        \n",
    "        if fit:\n",
    "            print(f\"[FinGAN] Normalized range: [{normalized.min():.4f}, {normalized.max():.4f}]\")\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "    def _denormalize_data(self, data):\n",
    "        data_range = self.data_max - self.data_min\n",
    "        return (data + 1) / 2 * data_range + self.data_min\n",
    "\n",
    "    def _make_windows(self, series, seq_len):\n",
    "        T = series.shape[0]\n",
    "        if T < seq_len:\n",
    "            raise ValueError(f\"Time series too short ({T}) for seq_len={seq_len}\")\n",
    "        \n",
    "        windows = []\n",
    "        for i in range(T - seq_len + 1):\n",
    "            windows.append(series[i:i+seq_len])\n",
    "        \n",
    "        return np.array(windows)\n",
    "\n",
    "    def _adjust_seq_len(self, data, factor=8):\n",
    "        n, seq_len, n_features = data.shape\n",
    "        target_len = (seq_len // factor) * factor\n",
    "        if target_len != seq_len:\n",
    "            print(f\"[FinGAN] Truncated seq_len {seq_len}  {target_len}\")\n",
    "        return data[:, :target_len, :]\n",
    "\n",
    "    def _build_models(self, seq_len, n_features):\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.G = FinGAN_Generator(\n",
    "            self.noise_dim, seq_len, n_features, self.base_channels\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.D = FinGAN_Critic(\n",
    "            seq_len, n_features, self.base_channels\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.opt_G = optim.Adam(self.G.parameters(), lr=self.lr_g, betas=(0.0, 0.9))\n",
    "        self.opt_D = optim.Adam(self.D.parameters(), lr=self.lr_d, betas=(0.0, 0.9))\n",
    "        \n",
    "        self._init_weights(self.G)\n",
    "        self._init_weights(self.D)\n",
    "        \n",
    "        print(f\"[FinGAN] Models built - G params: {sum(p.numel() for p in self.G.parameters()):,}\")\n",
    "        print(f\"[FinGAN] Models built - D params: {sum(p.numel() for p in self.D.parameters()):,}\")\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.BatchNorm1d, nn.LayerNorm)):\n",
    "            if hasattr(m, 'weight') and m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _gradient_penalty(self, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1, device=self.device)\n",
    "        \n",
    "        interp = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "        d_interp = self.D(interp)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interp,\n",
    "            inputs=interp,\n",
    "            grad_outputs=torch.ones_like(d_interp),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.reshape(batch_size, -1)\n",
    "        grad_norm = gradients.norm(2, dim=1)\n",
    "        penalty = torch.mean((grad_norm - 1) ** 2)\n",
    "        \n",
    "        return penalty\n",
    "\n",
    "    def train(self, train_data):\n",
    "        data = np.asarray(train_data, dtype=np.float32)\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "        \n",
    "        if np.isnan(data).any():\n",
    "            nan_count = np.isnan(data).sum()\n",
    "            print(f\"[WARNING] Found {nan_count} NaN values, handling...\")\n",
    "            data_df = pd.DataFrame(data)\n",
    "            data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "            data = data_df.values\n",
    "            print(f\"[FinGAN] NaN handling complete\")\n",
    "        \n",
    "        T, n_features = data.shape\n",
    "        \n",
    "        max_seq_len = (T // 8) * 8\n",
    "        if max_seq_len < 8:\n",
    "            raise ValueError(f\"Time series too short ({T}) for FIN-GAN\")\n",
    "        \n",
    "        if max_seq_len >= 256:\n",
    "            seq_len = 256\n",
    "        elif max_seq_len >= 128:\n",
    "            seq_len = 128\n",
    "        elif max_seq_len >= 64:\n",
    "            seq_len = 64\n",
    "        elif max_seq_len >= 32:\n",
    "            seq_len = 32\n",
    "        elif max_seq_len >= 16:\n",
    "            seq_len = 16\n",
    "        else:\n",
    "            seq_len = max_seq_len\n",
    "        \n",
    "        print(f\"[FinGAN] Creating windows: T={T}, seq_len={seq_len}, n_features={n_features}\")\n",
    "        \n",
    "        data = self._make_windows(data, seq_len)\n",
    "        \n",
    "        if np.isnan(data).any():\n",
    "            raise ValueError(\"NaN after windowing\")\n",
    "        \n",
    "        data = self._normalize_data(data, fit=True)\n",
    "        \n",
    "        if np.isnan(data).any():\n",
    "            raise ValueError(\"NaN after normalization\")\n",
    "        \n",
    "        n_windows = data.shape[0]\n",
    "        print(f\"[FinGAN] Created {n_windows} windows\")\n",
    "\n",
    "        if self.G is None:\n",
    "            self._build_models(seq_len, n_features)\n",
    "\n",
    "        dataset = TensorDataset(torch.tensor(data, dtype=torch.float32))\n",
    "        loader = DataLoader(dataset, self.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        print(f\"[FinGAN] Starting training with {len(loader)} batches per epoch\")\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            d_losses = []\n",
    "            g_losses = []\n",
    "            gp_losses = []\n",
    "            \n",
    "            for batch_idx, (real,) in enumerate(loader):\n",
    "                real = real.to(self.device)\n",
    "                batch_size = real.size(0)\n",
    "                \n",
    "                if torch.isnan(real).any():\n",
    "                    print(f\"[ERROR] NaN in batch {batch_idx}\")\n",
    "                    continue\n",
    "\n",
    "                for critic_iter in range(self.n_critic):\n",
    "                    self.opt_D.zero_grad()\n",
    "                    \n",
    "                    z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        fake = self.G(z)\n",
    "                    \n",
    "                    d_real = self.D(real)\n",
    "                    d_fake = self.D(fake)\n",
    "                    \n",
    "                    d_loss = d_fake.mean() - d_real.mean()\n",
    "                    gp = self._gradient_penalty(real, fake)\n",
    "                    d_total = d_loss + self.lambda_gp * gp\n",
    "                    \n",
    "                    if torch.isnan(d_total) or torch.isinf(d_total) or abs(d_total.item()) > 1e6:\n",
    "                        print(f\"[ERROR] Extreme D loss at epoch {epoch+1}, batch {batch_idx}\")\n",
    "                        break\n",
    "                    \n",
    "                    d_total.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.D.parameters(), max_norm=0.5)\n",
    "                    self.opt_D.step()\n",
    "                \n",
    "                d_losses.append(d_total.item())\n",
    "                gp_losses.append(gp.item())\n",
    "\n",
    "                self.opt_G.zero_grad()\n",
    "                z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "                fake = self.G(z)\n",
    "                g_loss = -self.D(fake).mean()\n",
    "                \n",
    "                if torch.isnan(g_loss) or torch.isinf(g_loss) or abs(g_loss.item()) > 1e6:\n",
    "                    print(f\"[ERROR] Extreme G loss\")\n",
    "                    continue\n",
    "                \n",
    "                g_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.G.parameters(), max_norm=0.5)\n",
    "                self.opt_G.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "\n",
    "            if len(d_losses) == 0 or len(g_losses) == 0:\n",
    "                print(f\"[ERROR] No valid losses in epoch {epoch+1}\")\n",
    "                break\n",
    "                \n",
    "            avg_d = np.mean(d_losses)\n",
    "            avg_g = np.mean(g_losses)\n",
    "            avg_gp = np.mean(gp_losses)\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"[FinGAN] Epoch {epoch+1}/{self.epochs} | D={avg_d:.4f} | G={avg_g:.4f} | GP={avg_gp:.4f}\")\n",
    "\n",
    "    def validate(self, val_data):\n",
    "        data = np.asarray(val_data, dtype=np.float32)\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "        \n",
    "        if np.isnan(data).any():\n",
    "            data_df = pd.DataFrame(data)\n",
    "            data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "            data = data_df.values\n",
    "        \n",
    "        if self.seq_len is None:\n",
    "            raise RuntimeError(\"Model not trained yet\")\n",
    "        \n",
    "        T = data.shape[0]\n",
    "        \n",
    "        if T < self.seq_len:\n",
    "            temp_seq_len = (T // 8) * 8\n",
    "            if temp_seq_len < 8:\n",
    "                return 0.0\n",
    "            \n",
    "            data = self._make_windows(data, temp_seq_len)\n",
    "            data = self._normalize_data(data, fit=False)\n",
    "            \n",
    "            n_windows, _, n_features = data.shape\n",
    "            padded_data = np.zeros((n_windows, self.seq_len, n_features), dtype=np.float32)\n",
    "            padded_data[:, :temp_seq_len, :] = data\n",
    "            data = padded_data\n",
    "        else:\n",
    "            data = self._make_windows(data, self.seq_len)\n",
    "            data = self._normalize_data(data, fit=False)\n",
    "        \n",
    "        data = self._adjust_seq_len(data, factor=8)\n",
    "        real = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.G.eval()\n",
    "        self.D.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = min(self.batch_size, real.size(0))\n",
    "            z = torch.randn(batch_size, self.noise_dim, device=self.device)\n",
    "            fake = self.G(z)\n",
    "            score = (self.D(real[:batch_size]).mean() - self.D(fake).mean()).item()\n",
    "        \n",
    "        self.G.train()\n",
    "        self.D.train()\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate n_samples timesteps (not windows)\"\"\"\n",
    "        if self.G is None:\n",
    "            raise RuntimeError(\"Model not trained yet\")\n",
    "\n",
    "        self.G.eval()\n",
    "        \n",
    "        # Generate enough windows to cover n_samples timesteps\n",
    "        n_windows = max(1, (n_samples + self.seq_len - 1) // self.seq_len)\n",
    "        \n",
    "        out = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_windows, self.batch_size):\n",
    "                b = min(self.batch_size, n_windows - i)\n",
    "                z = torch.randn(b, self.noise_dim, device=self.device)\n",
    "                fake_windows = self.G(z).cpu().numpy()\n",
    "                out.append(fake_windows)\n",
    "\n",
    "        self.G.train()\n",
    "        windows = np.concatenate(out, axis=0)\n",
    "        \n",
    "        # Denormalize\n",
    "        windows = self._denormalize_data(windows)\n",
    "        \n",
    "        # Flatten windows into continuous time series\n",
    "        reconstructed = windows.reshape(-1, self.n_features)\n",
    "        \n",
    "        # Trim to exact n_samples\n",
    "        reconstructed = reconstructed[:n_samples]\n",
    "        \n",
    "        # Pad if needed\n",
    "        if reconstructed.shape[0] < n_samples:\n",
    "            padding = np.repeat(reconstructed[-1:], n_samples - reconstructed.shape[0], axis=0)\n",
    "            reconstructed = np.vstack([reconstructed, padding])\n",
    "        \n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f157787",
   "metadata": {},
   "source": [
    "Helper: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7ffda96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    print(f\"Loaded {path}: shape={df.shape}, NaN count={df.isna().sum().sum()}\")\n",
    "    return df.to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04904d",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "54a13589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(models, datasets):\n",
    "    for ds_path in datasets:\n",
    "        data = load_dataset(ds_path)\n",
    "        print(f\"\\n Training on {ds_path}\")\n",
    "\n",
    "        for model in models:\n",
    "            print(f\"--- {model.name()} ---\")\n",
    "            model.train(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b31de",
   "metadata": {},
   "source": [
    "Validation Loop (Select Hyperparameters)\n",
    "\n",
    "Goal: Tune hyperparameters and choose best config.\n",
    "\n",
    "You will later integrate Optuna or grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7ee17fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(models, datasets):\n",
    "    results = []\n",
    "\n",
    "    for ds_path in datasets:\n",
    "        data = load_dataset(ds_path)\n",
    "        print(f\"\\n Validating on {ds_path}\")\n",
    "\n",
    "        for model in models:\n",
    "            score = model.validate(data)\n",
    "            print(f\"{model.name()}  validation score: {score:.4f}\")\n",
    "\n",
    "            results.append({\n",
    "                \"dataset\": ds_path,\n",
    "                \"model\": model.name(),\n",
    "                \"score\": score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85aaf5",
   "metadata": {},
   "source": [
    "Test Loop (Final Comparison)\n",
    "\n",
    "This generates synthetic data and compares it to the real test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3734b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(models, datasets):\n",
    "    results = []\n",
    "\n",
    "    for ds_path in datasets:\n",
    "        real = load_dataset(ds_path)\n",
    "        print(f\"\\n Testing on {ds_path}\")\n",
    "\n",
    "        for model in models:\n",
    "            synthetic = model.generate(len(real))\n",
    "            metrics = compute_metrics(real, synthetic)\n",
    "            print(f\"{model.name()}: {metrics}\")\n",
    "\n",
    "            results.append({\n",
    "                \"dataset\": ds_path,\n",
    "                \"model\": model.name(),\n",
    "                **metrics\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d0dab",
   "metadata": {},
   "source": [
    "Run the Whole Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "98cc10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    TimeGAN(config={\"hidden_dim\": 24, \"noise_dim\": 24, \"num_layers\": 3, \"lr\": 1e-3, \"epochs\": 5, \"batch_size\": 128}),\n",
    "    QuantGAN(config={\"lr_g\": 1e-4, \"lr_d\": 1e-4, \"epochs\": 5, \"noise_dim\": 100, \"n_critic\": 3, \"lambda_gp\": 10.0}),\n",
    "    FinGAN(config={\"lr\": 5e-5, \"epochs\": 5})\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "91dcf5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/BOVESPA_train.parquet: shape=(996, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/BOVESPA_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=996, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.4368, 0.2922]\n",
      "[TimeGAN] Mean: -0.028796, Std: 0.128135\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 741 windows\n",
      "[TimeGAN] Models built:\n",
      "  Embedder: 9,744\n",
      "  Recovery: 10,825\n",
      "  Generator: 11,400\n",
      "  Supervisor: 7,800\n",
      "  Discriminator: 10,825\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/638888921.py:241: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/2 | Reconstruction Loss: 0.0316\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0037\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0037\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Epoch 1/5 | D=1.3861 | G=9.1771\n",
      "[TimeGAN] Epoch 1/5 | D=1.3861 | G=9.1771\n",
      "[TimeGAN] Epoch 5/5 | D=1.3837 | G=2.2344\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=996, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.4368, 0.2922]\n",
      "[QuantGAN] Mean: -0.028796, Std: 0.128135\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 741 windows\n",
      "[QuantGAN] Models built - G params: 483,329\n",
      "[QuantGAN] Models built - D params: 115,489\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n",
      "[TimeGAN] Epoch 5/5 | D=1.3837 | G=2.2344\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=996, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.4368, 0.2922]\n",
      "[QuantGAN] Mean: -0.028796, Std: 0.128135\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 741 windows\n",
      "[QuantGAN] Models built - G params: 483,329\n",
      "[QuantGAN] Models built - D params: 115,489\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/4271837166.py:301: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantGAN] Epoch 1/5 | D=7.7827 | G=0.3330 | GP=0.7796\n",
      "[QuantGAN] Epoch 5/5 | D=3.2050 | G=1.8577 | GP=0.3179\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=996, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.4368, 0.2922]\n",
      "[FinGAN] Mean: -0.028796, Std: 0.128135\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 741 windows\n",
      "[FinGAN] Models built - G params: 108,137\n",
      "[FinGAN] Models built - D params: 1,139,329\n",
      "[FinGAN] Starting training with 11 batches per epoch\n",
      "[QuantGAN] Epoch 5/5 | D=3.2050 | G=1.8577 | GP=0.3179\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=996, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.4368, 0.2922]\n",
      "[FinGAN] Mean: -0.028796, Std: 0.128135\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 741 windows\n",
      "[FinGAN] Models built - G params: 108,137\n",
      "[FinGAN] Models built - D params: 1,139,329\n",
      "[FinGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/677272123.py:220: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FinGAN] Epoch 1/5 | D=-8.8484 | G=4.6469 | GP=0.2021\n",
      "[FinGAN] Epoch 5/5 | D=-13.0147 | G=-1.5226 | GP=0.2848\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/FTSE_train.parquet: shape=(1000, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/FTSE_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=1000, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.6951, 0.3080]\n",
      "[TimeGAN] Mean: -0.024689, Std: 0.175459\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 745 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n",
      "[FinGAN] Epoch 5/5 | D=-13.0147 | G=-1.5226 | GP=0.2848\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/FTSE_train.parquet: shape=(1000, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/FTSE_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=1000, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.6951, 0.3080]\n",
      "[TimeGAN] Mean: -0.024689, Std: 0.175459\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 745 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/638888921.py:241: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/2 | Reconstruction Loss: 0.0428\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0001\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0001\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Epoch 1/5 | D=1.3886 | G=1.9422\n",
      "[TimeGAN] Epoch 1/5 | D=1.3886 | G=1.9422\n",
      "[TimeGAN] Epoch 5/5 | D=1.3847 | G=1.2015\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=1000, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.6951, 0.3080]\n",
      "[QuantGAN] Mean: -0.024689, Std: 0.175459\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 745 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n",
      "[TimeGAN] Epoch 5/5 | D=1.3847 | G=1.2015\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=1000, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.6951, 0.3080]\n",
      "[QuantGAN] Mean: -0.024689, Std: 0.175459\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 745 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/4271837166.py:301: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantGAN] Epoch 1/5 | D=3.1752 | G=2.2139 | GP=0.3078\n",
      "[QuantGAN] Epoch 5/5 | D=1.2045 | G=3.6163 | GP=0.1015\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=1000, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.6951, 0.3080]\n",
      "[FinGAN] Mean: -0.024689, Std: 0.175459\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 745 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n",
      "[QuantGAN] Epoch 5/5 | D=1.2045 | G=3.6163 | GP=0.1015\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=1000, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.6951, 0.3080]\n",
      "[FinGAN] Mean: -0.024689, Std: 0.175459\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 745 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/677272123.py:220: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FinGAN] Epoch 1/5 | D=-16.9868 | G=-1.7938 | GP=0.4414\n",
      "[FinGAN] Epoch 5/5 | D=-16.2646 | G=-8.8974 | GP=0.4276\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/MSCI_train.parquet: shape=(1004, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/MSCI_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=1004, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.5521, 0.6546]\n",
      "[TimeGAN] Mean: 0.006594, Std: 0.222930\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 749 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n",
      "[FinGAN] Epoch 5/5 | D=-16.2646 | G=-8.8974 | GP=0.4276\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/MSCI_train.parquet: shape=(1004, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/MSCI_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=1004, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.5521, 0.6546]\n",
      "[TimeGAN] Mean: 0.006594, Std: 0.222930\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 749 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/638888921.py:241: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/2 | Reconstruction Loss: 0.0486\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0000\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0000\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Epoch 1/5 | D=1.3862 | G=1.3938\n",
      "[TimeGAN] Epoch 1/5 | D=1.3862 | G=1.3938\n",
      "[TimeGAN] Epoch 5/5 | D=1.3859 | G=1.0149\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=1004, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.5521, 0.6546]\n",
      "[QuantGAN] Mean: 0.006594, Std: 0.222930\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 749 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n",
      "[TimeGAN] Epoch 5/5 | D=1.3859 | G=1.0149\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=1004, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.5521, 0.6546]\n",
      "[QuantGAN] Mean: 0.006594, Std: 0.222930\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 749 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/4271837166.py:301: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantGAN] Epoch 1/5 | D=0.7619 | G=3.9211 | GP=0.0824\n",
      "[QuantGAN] Epoch 5/5 | D=-1.6702 | G=6.8889 | GP=0.0663\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=1004, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.5521, 0.6546]\n",
      "[FinGAN] Mean: 0.006594, Std: 0.222930\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 749 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n",
      "[QuantGAN] Epoch 5/5 | D=-1.6702 | G=6.8889 | GP=0.0663\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=1004, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.5521, 0.6546]\n",
      "[FinGAN] Mean: 0.006594, Std: 0.222930\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 749 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/677272123.py:220: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FinGAN] Epoch 1/5 | D=-9.7452 | G=-11.5316 | GP=0.2048\n",
      "[FinGAN] Epoch 5/5 | D=-9.4836 | G=-21.6810 | GP=0.1859\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/NIFTY50_train.parquet: shape=(993, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/NIFTY50_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=993, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.7768, 0.5173]\n",
      "[TimeGAN] Mean: -0.013686, Std: 0.255715\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 738 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n",
      "[FinGAN] Epoch 5/5 | D=-9.4836 | G=-21.6810 | GP=0.1859\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/NIFTY50_train.parquet: shape=(993, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/NIFTY50_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=993, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.7768, 0.5173]\n",
      "[TimeGAN] Mean: -0.013686, Std: 0.255715\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 738 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/638888921.py:241: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/2 | Reconstruction Loss: 0.0352\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0000\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0000\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Epoch 1/5 | D=1.3864 | G=1.1437\n",
      "[TimeGAN] Epoch 1/5 | D=1.3864 | G=1.1437\n",
      "[TimeGAN] Epoch 5/5 | D=1.3863 | G=0.9599\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=993, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.7768, 0.5173]\n",
      "[QuantGAN] Mean: -0.013686, Std: 0.255715\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 738 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n",
      "[TimeGAN] Epoch 5/5 | D=1.3863 | G=0.9599\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=993, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.7768, 0.5173]\n",
      "[QuantGAN] Mean: -0.013686, Std: 0.255715\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 738 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/4271837166.py:301: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantGAN] Epoch 1/5 | D=-0.8934 | G=7.5089 | GP=0.0536\n",
      "[QuantGAN] Epoch 5/5 | D=0.0885 | G=4.9412 | GP=0.0291\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=993, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.7768, 0.5173]\n",
      "[FinGAN] Mean: -0.013686, Std: 0.255715\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 738 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n",
      "[QuantGAN] Epoch 5/5 | D=0.0885 | G=4.9412 | GP=0.0291\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=993, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.7768, 0.5173]\n",
      "[FinGAN] Mean: -0.013686, Std: 0.255715\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 738 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/677272123.py:220: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FinGAN] Epoch 1/5 | D=-12.4529 | G=-21.7716 | GP=0.2644\n",
      "[FinGAN] Epoch 5/5 | D=-11.7588 | G=-24.7964 | GP=0.2708\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/SHANGHAI_train.parquet: shape=(969, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/SHANGHAI_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=969, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.2997, 0.3034]\n",
      "[TimeGAN] Mean: -0.032045, Std: 0.087099\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 714 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n",
      "[FinGAN] Epoch 5/5 | D=-11.7588 | G=-24.7964 | GP=0.2708\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/SHANGHAI_train.parquet: shape=(969, 1), NaN count=1\n",
      "\n",
      " Training on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/train/SHANGHAI_train.parquet\n",
      "--- TimeGAN ---\n",
      "[TimeGAN] WARNING: Found 1 NaN values, handling...\n",
      "[TimeGAN] NaN handling complete\n",
      "[TimeGAN] Creating windows: T=969, seq_len=256, n_features=1\n",
      "[TimeGAN] Data range: [-0.2997, 0.3034]\n",
      "[TimeGAN] Mean: -0.032045, Std: 0.087099\n",
      "[TimeGAN] Normalized range: [0.0000, 1.0000]\n",
      "[TimeGAN] Created 714 windows\n",
      "[TimeGAN] Phase 1: Training Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/638888921.py:241: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/2 | Reconstruction Loss: 0.0399\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "[TimeGAN] Phase 2: Training Supervisor...\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0000\n",
      "  Epoch 1/2 | Supervisor Loss: 0.0000\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Phase 3: Joint Adversarial Training...\n",
      "[TimeGAN] Epoch 1/5 | D=1.3860 | G=1.1566\n",
      "[TimeGAN] Epoch 1/5 | D=1.3860 | G=1.1566\n",
      "[TimeGAN] Epoch 5/5 | D=1.3862 | G=0.9303\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=969, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.2997, 0.3034]\n",
      "[QuantGAN] Mean: -0.032045, Std: 0.087099\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 714 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n",
      "[TimeGAN] Epoch 5/5 | D=1.3862 | G=0.9303\n",
      "--- QuantGAN ---\n",
      "[QuantGAN] WARNING: Found 1 NaN values, handling...\n",
      "[QuantGAN] NaN handling complete\n",
      "[QuantGAN] Creating windows: T=969, seq_len=256, n_features=1\n",
      "[QuantGAN] Data range: [-0.2997, 0.3034]\n",
      "[QuantGAN] Mean: -0.032045, Std: 0.087099\n",
      "[QuantGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[QuantGAN] Created 714 windows\n",
      "[QuantGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/4271837166.py:301: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuantGAN] Epoch 1/5 | D=-0.4918 | G=3.7333 | GP=0.0217\n",
      "[QuantGAN] Epoch 5/5 | D=-0.8683 | G=2.5853 | GP=0.0421\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=969, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.2997, 0.3034]\n",
      "[FinGAN] Mean: -0.032045, Std: 0.087099\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 714 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n",
      "[QuantGAN] Epoch 5/5 | D=-0.8683 | G=2.5853 | GP=0.0421\n",
      "--- FinGAN ---\n",
      "[WARNING] Found 1 NaN values, handling...\n",
      "[FinGAN] NaN handling complete\n",
      "[FinGAN] Creating windows: T=969, seq_len=256, n_features=1\n",
      "[FinGAN] Data range: [-0.2997, 0.3034]\n",
      "[FinGAN] Mean: -0.032045, Std: 0.087099\n",
      "[FinGAN] Normalized range: [-0.9900, 0.9900]\n",
      "[FinGAN] Created 714 windows\n",
      "[FinGAN] Starting training with 11 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_180600/677272123.py:220: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_df = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FinGAN] Epoch 1/5 | D=-7.4301 | G=-25.2681 | GP=0.1448\n",
      "[FinGAN] Epoch 5/5 | D=-6.9999 | G=-31.1334 | GP=0.1294\n",
      "[FinGAN] Epoch 5/5 | D=-6.9999 | G=-31.1334 | GP=0.1294\n"
     ]
    }
   ],
   "source": [
    "train_all_models(models, train_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0c74f3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/BOVESPA_valid.parquet: shape=(125, 1), NaN count=0\n",
      "\n",
      " Validating on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/BOVESPA_valid.parquet\n",
      "TimeGAN  validation score: 0.5008\n",
      "QuantGAN  validation score: 3.5410\n",
      "FinGAN  validation score: 9.1565\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/FTSE_valid.parquet: shape=(125, 1), NaN count=0\n",
      "\n",
      " Validating on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/FTSE_valid.parquet\n",
      "TimeGAN  validation score: 0.5007\n",
      "QuantGAN  validation score: 4.8217\n",
      "FinGAN  validation score: 12.0362\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/MSCI_valid.parquet: shape=(126, 1), NaN count=0\n",
      "\n",
      " Validating on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/MSCI_valid.parquet\n",
      "TimeGAN  validation score: 0.5008\n",
      "QuantGAN  validation score: 4.5700\n",
      "FinGAN  validation score: 10.2109\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/NIFTY50_valid.parquet: shape=(124, 1), NaN count=0\n",
      "\n",
      " Validating on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/NIFTY50_valid.parquet\n",
      "TimeGAN  validation score: 0.5007\n",
      "QuantGAN  validation score: 6.8310\n",
      "FinGAN  validation score: 12.7999\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/SHANGHAI_valid.parquet: shape=(121, 1), NaN count=0\n",
      "\n",
      " Validating on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/SHANGHAI_valid.parquet\n",
      "TimeGAN  validation score: 0.5008\n",
      "QuantGAN  validation score: 3.8261\n",
      "FinGAN  validation score: 9.0020\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a54be92c-d50d-4d70-8e01-e809edad4a7f",
       "rows": [
        [
         "0",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/BOVESPA_valid.parquet",
         "TimeGAN",
         "0.5007928609848022"
        ],
        [
         "1",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/BOVESPA_valid.parquet",
         "QuantGAN",
         "3.5409631729125977"
        ],
        [
         "2",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/BOVESPA_valid.parquet",
         "FinGAN",
         "9.156475067138672"
        ],
        [
         "3",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/FTSE_valid.parquet",
         "TimeGAN",
         "0.5007460117340088"
        ],
        [
         "4",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/FTSE_valid.parquet",
         "QuantGAN",
         "4.821685791015625"
        ],
        [
         "5",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/FTSE_valid.parquet",
         "FinGAN",
         "12.036170959472656"
        ],
        [
         "6",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/MSCI_valid.parquet",
         "TimeGAN",
         "0.5007756948471069"
        ],
        [
         "7",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/MSCI_valid.parquet",
         "QuantGAN",
         "4.570028305053711"
        ],
        [
         "8",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/MSCI_valid.parquet",
         "FinGAN",
         "10.210884094238281"
        ],
        [
         "9",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/NIFTY50_valid.parquet",
         "TimeGAN",
         "0.500740110874176"
        ],
        [
         "10",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/NIFTY50_valid.parquet",
         "QuantGAN",
         "6.8310346603393555"
        ],
        [
         "11",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/NIFTY50_valid.parquet",
         "FinGAN",
         "12.799932479858398"
        ],
        [
         "12",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/SHANGHAI_valid.parquet",
         "TimeGAN",
         "0.500798225402832"
        ],
        [
         "13",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/SHANGHAI_valid.parquet",
         "QuantGAN",
         "3.826131820678711"
        ],
        [
         "14",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/valid/SHANGHAI_valid.parquet",
         "FinGAN",
         "9.00201416015625"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.500793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>3.540963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>9.156475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.500746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>4.821686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>12.036171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.500776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>4.570028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>10.210884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.500740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>6.831035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>12.799932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.500798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>3.826132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>9.002014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dataset     model      score\n",
       "0   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN   0.500793\n",
       "1   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN   3.540963\n",
       "2   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN   9.156475\n",
       "3   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN   0.500746\n",
       "4   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN   4.821686\n",
       "5   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  12.036171\n",
       "6   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN   0.500776\n",
       "7   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN   4.570028\n",
       "8   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  10.210884\n",
       "9   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN   0.500740\n",
       "10  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN   6.831035\n",
       "11  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  12.799932\n",
       "12  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN   0.500798\n",
       "13  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN   3.826132\n",
       "14  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN   9.002014"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_results = validate_models(models, val_datasets)\n",
    "val_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d8d733f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/BOVESPA_test.parquet: shape=(125, 1), NaN count=0\n",
      "\n",
      " Testing on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/BOVESPA_test.parquet\n",
      "TimeGAN: {'MSE': 0.005857334937900305, 'KS': np.float64(2.6054291453487907e-38)}\n",
      "QuantGAN: {'MSE': 0.05963267385959625, 'KS': np.float64(2.439741350869797e-61)}\n",
      "FinGAN: {'MSE': 0.04294469952583313, 'KS': np.float64(2.1799860619410366e-24)}\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/FTSE_test.parquet: shape=(125, 1), NaN count=0\n",
      "\n",
      " Testing on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/FTSE_test.parquet\n",
      "TimeGAN: {'MSE': 0.028821803629398346, 'KS': np.float64(2.1927812846103578e-74)}\n",
      "QuantGAN: {'MSE': 0.1302771270275116, 'KS': np.float64(5.481953211525894e-72)}\n",
      "FinGAN: {'MSE': 0.08702515065670013, 'KS': np.float64(2.439741350869797e-61)}\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/MSCI_test.parquet: shape=(126, 1), NaN count=0\n",
      "\n",
      " Testing on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/MSCI_test.parquet\n",
      "TimeGAN: {'MSE': 0.007010139990597963, 'KS': np.float64(4.577127976907555e-21)}\n",
      "QuantGAN: {'MSE': 0.06385568529367447, 'KS': np.float64(1.4505248197697535e-68)}\n",
      "FinGAN: {'MSE': 0.04639298468828201, 'KS': np.float64(6.836782143725009e-23)}\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/NIFTY50_test.parquet: shape=(125, 1), NaN count=0\n",
      "\n",
      " Testing on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/NIFTY50_test.parquet\n",
      "TimeGAN: {'MSE': 0.015401923097670078, 'KS': np.float64(1.158985542149682e-20)}\n",
      "QuantGAN: {'MSE': 0.050319403409957886, 'KS': np.float64(8.800696171082814e-16)}\n",
      "FinGAN: {'MSE': 0.06172822788357735, 'KS': np.float64(2.1541846156127775e-18)}\n",
      "Loaded /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/SHANGHAI_test.parquet: shape=(122, 1), NaN count=0\n",
      "\n",
      " Testing on /home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/SHANGHAI_test.parquet\n",
      "TimeGAN: {'MSE': 0.021581942215561867, 'KS': np.float64(2.3707767774562415e-55)}\n",
      "QuantGAN: {'MSE': 0.08542484045028687, 'KS': np.float64(8.487017366035801e-47)}\n",
      "FinGAN: {'MSE': 0.08228851109743118, 'KS': np.float64(3.4076400134636285e-34)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dataset",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "KS",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "778c7ff6-a112-4055-bbd9-ea99efc6ebe3",
       "rows": [
        [
         "0",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/BOVESPA_test.parquet",
         "TimeGAN",
         "0.005857334937900305",
         "2.6054291453487907e-38"
        ],
        [
         "1",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/BOVESPA_test.parquet",
         "QuantGAN",
         "0.05963267385959625",
         "2.439741350869797e-61"
        ],
        [
         "2",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/BOVESPA_test.parquet",
         "FinGAN",
         "0.04294469952583313",
         "2.1799860619410366e-24"
        ],
        [
         "3",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/FTSE_test.parquet",
         "TimeGAN",
         "0.028821803629398346",
         "2.1927812846103578e-74"
        ],
        [
         "4",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/FTSE_test.parquet",
         "QuantGAN",
         "0.1302771270275116",
         "5.481953211525894e-72"
        ],
        [
         "5",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/FTSE_test.parquet",
         "FinGAN",
         "0.08702515065670013",
         "2.439741350869797e-61"
        ],
        [
         "6",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/MSCI_test.parquet",
         "TimeGAN",
         "0.007010139990597963",
         "4.577127976907555e-21"
        ],
        [
         "7",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/MSCI_test.parquet",
         "QuantGAN",
         "0.06385568529367447",
         "1.4505248197697535e-68"
        ],
        [
         "8",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/MSCI_test.parquet",
         "FinGAN",
         "0.04639298468828201",
         "6.836782143725009e-23"
        ],
        [
         "9",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/NIFTY50_test.parquet",
         "TimeGAN",
         "0.015401923097670078",
         "1.158985542149682e-20"
        ],
        [
         "10",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/NIFTY50_test.parquet",
         "QuantGAN",
         "0.050319403409957886",
         "8.800696171082814e-16"
        ],
        [
         "11",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/NIFTY50_test.parquet",
         "FinGAN",
         "0.06172822788357735",
         "2.1541846156127775e-18"
        ],
        [
         "12",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/SHANGHAI_test.parquet",
         "TimeGAN",
         "0.021581942215561867",
         "2.3707767774562415e-55"
        ],
        [
         "13",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/SHANGHAI_test.parquet",
         "QuantGAN",
         "0.08542484045028687",
         "8.487017366035801e-47"
        ],
        [
         "14",
         "/home/sobottka/BSE/Master_Thesis/bse-thesis-synthetic-data/data/processed_files/test/SHANGHAI_test.parquet",
         "FinGAN",
         "0.08228851109743118",
         "3.4076400134636285e-34"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>2.605429e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>0.059633</td>\n",
       "      <td>2.439741e-61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>0.042945</td>\n",
       "      <td>2.179986e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.028822</td>\n",
       "      <td>2.192781e-74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>0.130277</td>\n",
       "      <td>5.481953e-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>0.087025</td>\n",
       "      <td>2.439741e-61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>4.577128e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>0.063856</td>\n",
       "      <td>1.450525e-68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>0.046393</td>\n",
       "      <td>6.836782e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.015402</td>\n",
       "      <td>1.158986e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>0.050319</td>\n",
       "      <td>8.800696e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>2.154185e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>TimeGAN</td>\n",
       "      <td>0.021582</td>\n",
       "      <td>2.370777e-55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>QuantGAN</td>\n",
       "      <td>0.085425</td>\n",
       "      <td>8.487017e-47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/home/sobottka/BSE/Master_Thesis/bse-thesis-sy...</td>\n",
       "      <td>FinGAN</td>\n",
       "      <td>0.082289</td>\n",
       "      <td>3.407640e-34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dataset     model       MSE  \\\n",
       "0   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN  0.005857   \n",
       "1   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN  0.059633   \n",
       "2   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  0.042945   \n",
       "3   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN  0.028822   \n",
       "4   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN  0.130277   \n",
       "5   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  0.087025   \n",
       "6   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN  0.007010   \n",
       "7   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN  0.063856   \n",
       "8   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  0.046393   \n",
       "9   /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN  0.015402   \n",
       "10  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN  0.050319   \n",
       "11  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  0.061728   \n",
       "12  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...   TimeGAN  0.021582   \n",
       "13  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...  QuantGAN  0.085425   \n",
       "14  /home/sobottka/BSE/Master_Thesis/bse-thesis-sy...    FinGAN  0.082289   \n",
       "\n",
       "              KS  \n",
       "0   2.605429e-38  \n",
       "1   2.439741e-61  \n",
       "2   2.179986e-24  \n",
       "3   2.192781e-74  \n",
       "4   5.481953e-72  \n",
       "5   2.439741e-61  \n",
       "6   4.577128e-21  \n",
       "7   1.450525e-68  \n",
       "8   6.836782e-23  \n",
       "9   1.158986e-20  \n",
       "10  8.800696e-16  \n",
       "11  2.154185e-18  \n",
       "12  2.370777e-55  \n",
       "13  8.487017e-47  \n",
       "14  3.407640e-34  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = test_models(models, test_datasets)\n",
    "test_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
