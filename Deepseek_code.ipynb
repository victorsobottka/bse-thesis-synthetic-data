{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to write a Python function to calculate GARCH volatility. Hmm, where do I start? I remember that GARCH stands for Generalized Autoregressive Conditional Heteroskedasticity. It's used to model the volatility of financial returns. So, the function should take some time series data as input and output the volatility measures.\n",
      "\n",
      "First, I think I need to import some necessary libraries. Pandas is probably needed for handling the data, and maybe matplotlib for plotting, but I'm not sure if that's necessary right now. NumPy might be useful for numerical operations too.\n",
      "\n",
      "Wait, what are the inputs? The user didn't specify, but perhaps the function should accept a pandas DataFrame with closing prices or log returns. Let me assume it's a Series of log returns since GARCH models are often applied to returns rather than prices. I'll need to check if the data is a Series and has at least 22 observations because GARCH requires a lag of at least one period.\n",
      "\n",
      "Next, parameters. The function should have parameters like the lag length (p), which is an integer greater than or equal to 1. The decay factor, alpha, usually between 0 and 1. Some implementations use different methods like Bollinger-Garner ringers, so maybe there's a 'method' parameter with options like 'BGS' for Bollinger-Garner single-step and 'TSM' for Tsay's multi-step.\n",
      "\n",
      "Inside the function, I'll need to compute the log returns. So, take the input series, calculate the differences between consecutive terms to get the returns. Then, apply the GARCH model using the chosen method.\n",
      "\n",
      "If the method is Bollinger-Garner single-step, I recall that the formula involves an average of squared residuals from previous periods. For each period t, it's 1 - alpha times the sum of squared residuals up to t-1 divided by (t-1) plus alpha times the latest squared residual.\n",
      "\n",
      "For Tsay's multi-step, the formula is a bit different. It uses the decay factor and previous heteroskedasticity terms. The formula would involve both past returns and their lags multiplied by the decay factor.\n",
      "\n",
      "I'll need to loop through each period, calculate the volatility at each step, and keep track of the sum of squared residuals for BGS or the necessary components for TSM.\n",
      "\n",
      "Once all volatilities are computed, I should return them as a Series. Maybe also include the mean of the volatilities as a measure of overall volatility.\n",
      "\n",
      "Let me outline the steps:\n",
      "1. Validate input: check if it's a Series with sufficient length.\n",
      "2. Compute log returns.\n",
      "3. Initialize necessary variables for each method.\n",
      "4. Loop through each period, compute the current volatility using the method.\n",
      "5. Update the necessary variables (sum of squared residuals or past volatilities).\n",
      "6. After computing all volatilities, calculate the mean.\n",
      "7. Return both the volatility series and the mean.\n",
      "\n",
      "Wait, I should make sure that for BGS, the initial sum is set correctly. For t=1, since there's no previous data, the volatility is just alpha times the squared residual. Then for each subsequent period, it uses the formula with 1 - alpha multiplied by the average of past residuals plus alpha times the current one.\n",
      "\n",
      "For Tsay's method, I need to keep track of the weighted sum of squared residuals and the weighted sum of lagged squared residuals. The decay factor is applied progressively as we move forward in time.\n",
      "\n",
      "I think that's the general approach. Now, I'll try to code this step by step, making sure each part works correctly.\n",
      "</think>\n",
      "\n",
      "To calculate GARCH volatility using Python, follow these steps:\n",
      "\n",
      "### Step-by-Step Explanation and Code\n",
      "\n",
      "1. **Import Necessary Libraries**: Use Pandas for data handling and NumPy for numerical operations.\n",
      "\n",
      "2. **Define the Function**:\n",
      "   - Parameters: `series` (pandas Series), `lag_length` (integer ≥1), `decay_factor`, `method` ('BGS' or 'TSM').\n",
      "   \n",
      "3. **Input Validation**: Ensure input is a valid pandas Series with at least 22 observations.\n",
      "\n",
      "4. **Compute Log Returns**:\n",
      "   - Calculate the log returns of the series.\n",
      "\n",
      "5. **Initialize Variables Based on Method**:\n",
      "   - For Bollinger-Garner Single-Step (BGS): Initialize `sum_sqres` to 0 and a list to store volatilities.\n",
      "   - For Tsay's Multi-Step (TSM): Initialize `weighted_sum_sqres`, `weighted_sum_lagged_sqres`, and the volatility list.\n",
      "\n",
      "6. **Iterate Through Periods**:\n",
      "   - Compute the squared residual for each period.\n",
      "   - Apply the GARCH formula based on the selected method to get the current volatility.\n",
      "\n",
      "7. **Update Variables**:\n",
      "   - For BGS: Update `sum_sqres` with the current squared residual.\n",
      "   - For TSM: Update the weighted sums accordingly, applying the decay factor progressively.\n",
      "\n",
      "8. **Calculate Overall Volatility**: Compute the mean of the volatility series.\n",
      "\n",
      "9. **Return Results**: Return both the volatility series and the mean.\n",
      "\n",
      "Here's the code implementing these steps:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def calculate_garch_volatility(series, lag_length=1, decay_factor=0.95, method='BGS'):\n",
      "    \"\"\"\n",
      "    Calculate GARCH (Generalized Autoregressive Conditional Heteroskedasticity) volatility.\n",
      "    \n",
      "    Args:\n",
      "        series (pd.Series): Time series data of log returns\n",
      "        lag_length (int, optional): Number of lags to consider (default: 1)\n",
      "        decay_factor (float, optional): Decay factor for GARCH model (default: 0.95)\n",
      "        method (str, optional): GARCH method, can be 'BGS' (Bollinger-Garner Single-Step) or \n",
      "                              'TSM' (Tsay's Multi-Step) (default: 'BGS')\n",
      "    \n",
      "    Returns:\n",
      "        dict: {'volatility': pd.Series, 'mean_vol': float}\n",
      "    \"\"\"\n",
      "    # Validate input\n",
      "    if not isinstance(series, pd.Series):\n",
      "        raise TypeError(\"Input must be a pandas Series\")\n",
      "    if len(series) < 22:\n",
      "        raise ValueError(\"Series must have at least 22 observations for GARCH analysis\")\n",
      "    \n",
      "    # Compute log returns\n",
      "    returns = series.pct_change().dropna()\n",
      "    \n",
      "    if method == 'BGS':\n",
      "        vol = []\n",
      "        sum_sqres = 0\n",
      "        for i in range(len(returns)):\n",
      "            if i < lag_length:\n",
      "                current_sqr = np.sqrt(np.exp(2 * (i + 1) * np.log(abs(returns[i])))\n",
      "                # For first few periods, use alpha * current_sqr\n",
      "                vol.append(current_sqr * decay_factor)\n",
      "                sum_sqres += current_sqr ** 2\n",
      "            else:\n",
      "                avg_part = (sum_sqres / i) * (1 - decay_factor)\n",
      "                current_vol = np.sqrt(avg_part + (current_sqr ** 2) * decay_factor)\n",
      "                vol.append(current_vol)\n",
      "                sum_sqres += current_sqr ** 2\n",
      "        mean_vol = np.mean(vol)\n",
      "    elif method == 'TSM':\n",
      "        # Initialize weights\n",
      "        weighted_sum_sqres = 0\n",
      "        weighted_sum_lagged_sqres = 0\n",
      "        vol = []\n",
      "        \n",
      "        for i in range(len(returns)):\n",
      "            if i < lag_length:\n",
      "                current_sqr = np.sqrt(np.exp(2 * (i + 1) * np.log(abs(returns[i])))\n",
      "                # For first few periods, just use alpha * current_sqr\n",
      "                weighted_sum_sqres += decay_factor ** (i + 1) * current_sqr ** 2\n",
      "                vol.append(current_sqr)\n",
      "            else:\n",
      "                # Current term is lagged by lag_length\n",
      "                current_lag = np.sqrt(np.exp(2 * (lag_length + 1 - i) * np.log(abs(returns[i])))\n",
      "                # Sum previous weighted terms\n",
      "                weighted_sum_sqres += decay_factor ** (i - lag_length) * current_lag ** 2\n",
      "                weighted_sum_lagged_sqres += decay_factor ** (i - lag_length) * (current_sqr ** 2)\n",
      "                \n",
      "                # Compute volatility\n",
      "                avg_part = (weighted_sum_sqres - weighted_sum_lagged_sqres) * (1 - decay_factor)\n",
      "                current_vol = np.sqrt(avg_part + decay_factor)\n",
      "                vol.append(current_vol)\n",
      "        mean_vol = np.mean(vol)\n",
      "    else:\n",
      "        raise ValueError(\"Method must be 'BGS' or 'TSM'\")\n",
      "    \n",
      "    return {'volatility': pd.Series(vol), 'mean_vol': mean_vol}\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "- **Input Validation**: Ensures the function receives valid data.\n",
      "- **Log Returns**: Converts price changes into log returns for GARCH model applicability.\n",
      "- **GARCH Models**:\n",
      "  - **BGS (Bollinger-Garner Single-Step)**: Uses a simple average of squared residuals with a decay factor.\n",
      "  - **TSM (Tsay's Multi-Step)**: Applies a weighted sum approach, considering lagged terms.\n",
      "- **Decay Factor**: Controls the smoothness of the volatility estimate.\n",
      "- **Output**: Returns both the estimated volatility series and the mean for overall volatility assessment.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"deepseek-r1:8b\",  # match the model you pulled\n",
    "    \"prompt\": \"Write a Python function to calculate GARCH volatility.\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "print(response.json()[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load model with memory optimizations\n",
    "model_id = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Configure quantization with CPU offloading enabled\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading\n",
    ")\n",
    "\n",
    "# Load the model with quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Apply LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 307.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a small dummy dataset\n",
    "data = {\n",
    "    \"quote\": [\n",
    "        \"The sky is not the limit; it’s just the beginning.\",\n",
    "        \"Data is the new oil, but it needs refining.\",\n",
    "        \"AI is not replacing humans, it’s augmenting them.\",\n",
    "        \"Mistakes are proof that you are trying.\",\n",
    "        \"Stay curious, stay learning, stay relevant.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"quote\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "tokenized = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure training with memory-efficient settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|██████████| 5/5 [00:00<00:00, 3335.17 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You can't train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. If you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Use SFTTrainer with additional memory optimizations\u001b[39;00m\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     29\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m            \u001b[38;5;66;03m# Limit sequence length if possible\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Dump memory snapshot for analysis\u001b[39;00m\n\u001b[1;32m     38\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39m_dump_snapshot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_snapshot.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis_new/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis_new/lib/python3.10/site-packages/transformers/trainer.py:2369\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2367\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   2368\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2369\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2371\u001b[0m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m     model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m   2373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[1;32m   2374\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis_new/lib/python3.10/site-packages/accelerate/accelerator.py:1446\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1445\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m-> 1446\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_fix_optimizer:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis_new/lib/python3.10/site-packages/accelerate/accelerator.py:1447\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1445\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1446\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1447\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1448\u001b[0m     )\n\u001b[1;32m   1449\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_fix_optimizer:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis_new/lib/python3.10/site-packages/accelerate/accelerator.py:1289\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m-> 1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m   1291\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis_new/lib/python3.10/site-packages/accelerate/accelerator.py:1568\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1559\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1560\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1561\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.cuda.current_device()}` or `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.xpu.current_device()}`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m                 )\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_multi_backend_available())\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m is_xpu_available())\n\u001b[1;32m   1566\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices\n\u001b[1;32m   1567\u001b[0m     ):\n\u001b[0;32m-> 1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m         )\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_placement \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(model):\n\u001b[1;32m   1573\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: You can't train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. If you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Memory recording for debugging\n",
    "torch.cuda.memory._record_memory_history()\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Very memory efficient training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora-deepseek\",\n",
    "    per_device_train_batch_size=1,  # Minimize batch size\n",
    "    gradient_accumulation_steps=4,  # Accumulate instead\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",  # Use standard optimizer\n",
    "    max_grad_norm=0.3,    # Limit gradient size\n",
    "    warmup_ratio=0.03,    # Warm up learning rate\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_total_limit=3,   # Limit checkpoints\n",
    ")\n",
    "\n",
    "# Use SFTTrainer with additional memory optimizations\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized,\n",
    "    args=args,\n",
    "    peft_config=lora_config,  # Pass LoRA config here as well\n",
    "           # Limit sequence length if possible\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Dump memory snapshot for analysis\n",
    "torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
